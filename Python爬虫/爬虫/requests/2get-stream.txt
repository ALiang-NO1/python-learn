默认情况下是stream=false，他会立即开始下载文件并存放到内存当中，倘若文件过大就会导致内存不足的情况．
当下载大的文件的时候，建议使用stream模式．

当把get函数的stream参数设置成True时，它不会立即开始下载，当你使用iter_content或iter_lines遍历内容或访问内容属性时才开始下载。
需要注意一点：文件没有下载之前，它也需要保持连接。这里就用到了另一个巧妙的库了: contextlib.closing
iter_content：一块一块的遍历要下载的内容
iter_lines：一行一行的遍历要下载的内容
使用上面两个函数下载大文件可以防止占用过多的内存，因为每次只下载小部分数据。

示例代码：
r = requests.get(url_file, stream=True)
f = open("file_path", "wb")
for chunk in r.iter_content(chunk_size=512):    # 按照块的大小读取
# for chunk in r.iter_lines():    # 按照一行一行的读取
    if chunk:
        f.write(chunk)
 
更详细的介绍:
经过排查发现，是因为很多URL，实际是下载链接，会触发文件下载，这些URL对应的html中根本不会包含
这样的话,只有headers头被下载了，body中的数据还没有被下载，这样就能避免不必要的流量开销，只有当你使用r.content 的时候，所有body内容才会被下载

实时上还可以使用Response.iter_content() Response.iter_lines()
Response.raw()来自己决定要读取多少数据

最后要注意的是，使用stream=True以后需要自己执行Response的关闭操作
改进后的程序:
import logging
import threading
import redis
import requests
from lxml.html import fromstring

r = redis.Redis(host='127.0.0.1', port=6379, db=10)
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                    filename='all.log',
                    filemode='w')
def extract(url):
    logger = logging.getLogger()
    try:
        res = requests.get(url, stream=True, timeout=0.5)
        ctype = res.headers['Content-Type']
        ctype = ctype.lower()
        if ctype.find('text/html') == -1:
            res.close()
            return None

        doc = fromstring(res.content)
        res.close()
        item_list  = doc.xpath('//head/title')
        if item_list:
            res = item_list[0].text_content()
            res = unicode(res)
            logger.info('title = %s', res)
            return res
    except:
        return None
    return None